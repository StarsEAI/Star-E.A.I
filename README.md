# Star-E.A.I.
Work in progress. Unfortunately, this project's capabilities are limited by insufficient computational resources. In simpler terms, I can't train this model because I keep facing "out-of-memory" errors. E.A.I. stands for "Evolutionary Artificial Intelligence", and this project started with the incentive of innovating in the ML sphere.

NOTE: THIS IS A D.I.Y. GPT MODEL

Particularly, this ML model holds the unique ability to evolve it's architecture, it's neural network. I myself created an optimizer, similar to Adam, except one which adjusts the neural network of the model over epochs. Additionally, other than purely optimization techniques during training, I wanted to implement this custom optimizer, which I called "EVO" (short for "evolution"), in everyday usage, so that this GPT model had the ability to evolve and adapt its architecture over time, specifically to adapt to the user's interests.

The project in its entirety was supposed to be a desktop application, which basically worked like Github copilot, but with more features, such as the ability to search online, think more analytically, and not merely complete code blocks and have long conversations, but also the ability to innovately brainstorm on projects developers may have been working on. And with the previously described evolving neural network, this was supposed to be a masterpiece. Unfortunate to see such a roadblock - just not enough memory (RAM).
I purchased a google Colab pro+ subscription, hoping that that would provide me with GPUs better than my PC, and it did, but didn't work quite well. I still faced OOM (out-of-memory) errors, on the very first example of the very first batch of the training dataset. Even with an A100 GPU (which on its own has 40GB RAM), and 80GB system RAM, my program would still crash. Specifically, the errors encountered are: "failed to allocate tensor.." or "OOM when allocating tensor of shape..".

The result of this project was also meant to be commericialized. After releasing this desktop app and having it be free and opensource for about two months (just to see who would be interested in seeing how it works), I would put it up for a price, for a monthly subscription of $19.99/month. This would be an app that could be downloaded on a website that I would host with a domain.

Another interesting idea I had for this A.I. system is how it would manage long conversations. As far as I know, GPTs of today are limited by context windows. These windows are very large nowadays, e.g. 32k tokens for GPT4. Before beginning to get into the transformer architecture of the model, I had already known that computational resources would be a limitation, and in hopes of overcoming that, I implemented a system called "DCWP", which deciphers itself as "dynamic context window placement". Essentially, the idea was that rather than constantly keeping a large amount of tokens in the context window (as that wouldn't be possible due to computational limitations), the model should have a sort of "preprocessing" layer before the transformer architecture, which recorded "memory units". Simply put, after each user prompt, the model would store that prompt in a .txt file. 10 of such "stores" or "memory units" max. Then, with each and every new prompt, a pre-built sentence similarity would check the similarity of the new prompt with the recorded memory units. It would compare how contextually relevant the prompt is to each memory unit, and select the memory unit with highest relevance and one that reaches a certain threshold (e.g. "find me a memory unit which is the most relevant and scores a relevance scores higher than, say, 0.6"). Then, this memory unit would be plopped into the context window. The point of this whole thing, is when the user asks, say, "but what about that other thing, with Tensorflow's dynamic computation graph?", for the measely 512 token context window GPT model of mine to be able to still go back into the history of the conversation and, despite its context window, find out what the user is talking about and generate a comprehensive output to that prompt.

Unfortunately, the "DCWP" shall not be tested at this current moment, same as the "EVO" optimizer and the evolving architecture attribute of the model, due to OOM errors.

This project certainly isn't abandoned, rather, what you're reading right now is a journal entry of the current progression of this project. I'll see what I can do with this, and if you have ideas, please suggest some. Remember, error is OOM errors. 
